{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = torch.FloatTensor([[1, 2, 1],\n",
    "                             [1, 3, 2],\n",
    "                             [1, 3, 4],\n",
    "                             [1, 5, 5],\n",
    "                             [1, 7, 5],\n",
    "                             [1, 2, 5],\n",
    "                             [1, 6, 6],\n",
    "                             [1, 7, 7]\n",
    "                            ])\n",
    "y_train = torch.LongTensor([2, 2, 2, 1, 1, 1, 0, 0])\n",
    "\n",
    "x_test = torch.FloatTensor([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\n",
    "y_test = torch.LongTensor([2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,3)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = SoftmaxClassifierModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs + 1):\n",
    "        prediction = model(x_train)\n",
    "        cost = F.cross_entropy(prediction, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, x_test, y_test):\n",
    "    prediction = model(x_test)\n",
    "    predicted_classes = prediction.max(1)[1] # prediction 중에서 가장 큰 값을 내놓는다.\n",
    "    correct_count = (predicted_classes == y_test).sum().item()\n",
    "    cost = F.cross_entropy(prediction, y_test)\n",
    "    \n",
    "    print('Accuracy: {}% Cost: {:.6f}'.format(\n",
    "         correct_count / len(y_test) * 100, cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 2.203666\n",
      "Epoch    1/100 Cost: 1.989424\n",
      "Epoch    2/100 Cost: 1.803548\n",
      "Epoch    3/100 Cost: 1.647425\n",
      "Epoch    4/100 Cost: 1.520720\n",
      "Epoch    5/100 Cost: 1.421076\n",
      "Epoch    6/100 Cost: 1.344574\n",
      "Epoch    7/100 Cost: 1.286687\n",
      "Epoch    8/100 Cost: 1.243165\n",
      "Epoch    9/100 Cost: 1.210481\n",
      "Epoch   10/100 Cost: 1.185905\n",
      "Epoch   11/100 Cost: 1.167381\n",
      "Epoch   12/100 Cost: 1.153379\n",
      "Epoch   13/100 Cost: 1.142754\n",
      "Epoch   14/100 Cost: 1.134649\n",
      "Epoch   15/100 Cost: 1.128421\n",
      "Epoch   16/100 Cost: 1.123588\n",
      "Epoch   17/100 Cost: 1.119791\n",
      "Epoch   18/100 Cost: 1.116760\n",
      "Epoch   19/100 Cost: 1.114298\n",
      "Epoch   20/100 Cost: 1.112257\n",
      "Epoch   21/100 Cost: 1.110527\n",
      "Epoch   22/100 Cost: 1.109028\n",
      "Epoch   23/100 Cost: 1.107700\n",
      "Epoch   24/100 Cost: 1.106501\n",
      "Epoch   25/100 Cost: 1.105397\n",
      "Epoch   26/100 Cost: 1.104365\n",
      "Epoch   27/100 Cost: 1.103387\n",
      "Epoch   28/100 Cost: 1.102449\n",
      "Epoch   29/100 Cost: 1.101543\n",
      "Epoch   30/100 Cost: 1.100661\n",
      "Epoch   31/100 Cost: 1.099797\n",
      "Epoch   32/100 Cost: 1.098948\n",
      "Epoch   33/100 Cost: 1.098110\n",
      "Epoch   34/100 Cost: 1.097280\n",
      "Epoch   35/100 Cost: 1.096459\n",
      "Epoch   36/100 Cost: 1.095642\n",
      "Epoch   37/100 Cost: 1.094831\n",
      "Epoch   38/100 Cost: 1.094024\n",
      "Epoch   39/100 Cost: 1.093220\n",
      "Epoch   40/100 Cost: 1.092420\n",
      "Epoch   41/100 Cost: 1.091622\n",
      "Epoch   42/100 Cost: 1.090827\n",
      "Epoch   43/100 Cost: 1.090034\n",
      "Epoch   44/100 Cost: 1.089243\n",
      "Epoch   45/100 Cost: 1.088454\n",
      "Epoch   46/100 Cost: 1.087666\n",
      "Epoch   47/100 Cost: 1.086881\n",
      "Epoch   48/100 Cost: 1.086097\n",
      "Epoch   49/100 Cost: 1.085315\n",
      "Epoch   50/100 Cost: 1.084535\n",
      "Epoch   51/100 Cost: 1.083757\n",
      "Epoch   52/100 Cost: 1.082979\n",
      "Epoch   53/100 Cost: 1.082204\n",
      "Epoch   54/100 Cost: 1.081430\n",
      "Epoch   55/100 Cost: 1.080657\n",
      "Epoch   56/100 Cost: 1.079887\n",
      "Epoch   57/100 Cost: 1.079118\n",
      "Epoch   58/100 Cost: 1.078350\n",
      "Epoch   59/100 Cost: 1.077583\n",
      "Epoch   60/100 Cost: 1.076819\n",
      "Epoch   61/100 Cost: 1.076055\n",
      "Epoch   62/100 Cost: 1.075294\n",
      "Epoch   63/100 Cost: 1.074533\n",
      "Epoch   64/100 Cost: 1.073775\n",
      "Epoch   65/100 Cost: 1.073017\n",
      "Epoch   66/100 Cost: 1.072262\n",
      "Epoch   67/100 Cost: 1.071507\n",
      "Epoch   68/100 Cost: 1.070755\n",
      "Epoch   69/100 Cost: 1.070003\n",
      "Epoch   70/100 Cost: 1.069253\n",
      "Epoch   71/100 Cost: 1.068505\n",
      "Epoch   72/100 Cost: 1.067758\n",
      "Epoch   73/100 Cost: 1.067013\n",
      "Epoch   74/100 Cost: 1.066269\n",
      "Epoch   75/100 Cost: 1.065526\n",
      "Epoch   76/100 Cost: 1.064785\n",
      "Epoch   77/100 Cost: 1.064045\n",
      "Epoch   78/100 Cost: 1.063307\n",
      "Epoch   79/100 Cost: 1.062570\n",
      "Epoch   80/100 Cost: 1.061835\n",
      "Epoch   81/100 Cost: 1.061101\n",
      "Epoch   82/100 Cost: 1.060368\n",
      "Epoch   83/100 Cost: 1.059637\n",
      "Epoch   84/100 Cost: 1.058907\n",
      "Epoch   85/100 Cost: 1.058179\n",
      "Epoch   86/100 Cost: 1.057452\n",
      "Epoch   87/100 Cost: 1.056726\n",
      "Epoch   88/100 Cost: 1.056002\n",
      "Epoch   89/100 Cost: 1.055280\n",
      "Epoch   90/100 Cost: 1.054558\n",
      "Epoch   91/100 Cost: 1.053838\n",
      "Epoch   92/100 Cost: 1.053120\n",
      "Epoch   93/100 Cost: 1.052403\n",
      "Epoch   94/100 Cost: 1.051687\n",
      "Epoch   95/100 Cost: 1.050973\n",
      "Epoch   96/100 Cost: 1.050260\n",
      "Epoch   97/100 Cost: 1.049548\n",
      "Epoch   98/100 Cost: 1.048838\n",
      "Epoch   99/100 Cost: 1.048129\n",
      "Epoch  100/100 Cost: 1.047422\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, x_train, y_train)\n",
    "\n",
    "\n",
    "# Epoch    0/100 Cost: 2.203666\n",
    "# Epoch    1/100 Cost: 1.989424\n",
    "# Epoch    2/100 Cost: 1.803548\n",
    "# Epoch    3/100 Cost: 1.647425\n",
    "# Epoch    4/100 Cost: 1.520720\n",
    "# Epoch    5/100 Cost: 1.421076\n",
    "# Epoch    6/100 Cost: 1.344574\n",
    "# Epoch    7/100 Cost: 1.286687\n",
    "# Epoch    8/100 Cost: 1.243165\n",
    "# Epoch    9/100 Cost: 1.210481\n",
    "# Epoch   10/100 Cost: 1.185905\n",
    "# Epoch   11/100 Cost: 1.167381\n",
    "# Epoch   12/100 Cost: 1.153379\n",
    "# Epoch   13/100 Cost: 1.142754\n",
    "# Epoch   14/100 Cost: 1.134649\n",
    "# Epoch   15/100 Cost: 1.128421\n",
    "# Epoch   16/100 Cost: 1.123588\n",
    "# Epoch   17/100 Cost: 1.119791\n",
    "# Epoch   18/100 Cost: 1.116760\n",
    "# Epoch   19/100 Cost: 1.114298\n",
    "# Epoch   20/100 Cost: 1.112257\n",
    "# Epoch   21/100 Cost: 1.110527\n",
    "# Epoch   22/100 Cost: 1.109028\n",
    "# Epoch   23/100 Cost: 1.107700\n",
    "# Epoch   24/100 Cost: 1.106501\n",
    "# Epoch   96/100 Cost: 1.050260\n",
    "# Epoch   97/100 Cost: 1.049548\n",
    "# Epoch   98/100 Cost: 1.048838\n",
    "# Epoch   99/100 Cost: 1.048129\n",
    "# Epoch  100/100 Cost: 1.047422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0% Cost: 1.861790\n"
     ]
    }
   ],
   "source": [
    "test(model, x_test, y_test)\n",
    "\n",
    "# Accuracy: 0.0% Cost: 1.861790"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보면 이미 overfitting이 일어난 상태임을 알 수 있다.\n",
    "\n",
    "![](./image/2022-01-17-21-36-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate\n",
    "\n",
    "$\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(x;\\theta)$\n",
    "\n",
    "Learning Rate가 너무 크면 다음과 같이 발산한다.\n",
    "\n",
    "![](./image/2022-01-17-21-39-00.png)\n",
    "\n",
    "너무 작으면 아예 학습이 안되고."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "데이터를 전처리하는 과정을 통해서 문제를 보다 안정적으로 풀 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규분포화 한다.\n",
    "\n",
    "![](./image/2022-01-17-21-44-13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0674, -0.3758, -0.8398],\n",
      "        [ 0.7418,  0.2778,  0.5863],\n",
      "        [ 0.3799,  0.5229,  0.3486],\n",
      "        [ 1.0132,  1.0948,  1.1409],\n",
      "        [-1.0674, -1.5197, -1.2360]])\n"
     ]
    }
   ],
   "source": [
    "mu = x_train.mean(dim = 0)\n",
    "sigma = x_train.std(dim = 0)\n",
    "norm_x_train = (x_train - mu) / sigma\n",
    "print(norm_x_train) # gaussian distribution을 따른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLienarRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = MultivariateLienarRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, x_train, y_train):\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs + 1):\n",
    "        prediction = model(x_train)\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 29681.355469\n",
      "Epoch    1/100 Cost: 28484.753906\n",
      "Epoch    2/100 Cost: 27337.447266\n",
      "Epoch    3/100 Cost: 26237.316406\n",
      "Epoch    4/100 Cost: 25182.328125\n",
      "Epoch    5/100 Cost: 24170.564453\n",
      "Epoch    6/100 Cost: 23200.175781\n",
      "Epoch    7/100 Cost: 22269.408203\n",
      "Epoch    8/100 Cost: 21376.582031\n",
      "Epoch    9/100 Cost: 20520.101562\n",
      "Epoch   10/100 Cost: 19698.441406\n",
      "Epoch   11/100 Cost: 18910.128906\n",
      "Epoch   12/100 Cost: 18153.785156\n",
      "Epoch   13/100 Cost: 17428.066406\n",
      "Epoch   14/100 Cost: 16731.705078\n",
      "Epoch   15/100 Cost: 16063.479492\n",
      "Epoch   16/100 Cost: 15422.226562\n",
      "Epoch   17/100 Cost: 14806.833008\n",
      "Epoch   18/100 Cost: 14216.231445\n",
      "Epoch   19/100 Cost: 13649.401367\n",
      "Epoch   20/100 Cost: 13105.369141\n",
      "Epoch   21/100 Cost: 12583.198242\n",
      "Epoch   22/100 Cost: 12081.996094\n",
      "Epoch   23/100 Cost: 11600.904297\n",
      "Epoch   24/100 Cost: 11139.104492\n",
      "Epoch   25/100 Cost: 10695.811523\n",
      "Epoch   26/100 Cost: 10270.271484\n",
      "Epoch   27/100 Cost: 9861.763672\n",
      "Epoch   28/100 Cost: 9469.596680\n",
      "Epoch   29/100 Cost: 9093.109375\n",
      "Epoch   30/100 Cost: 8731.668945\n",
      "Epoch   31/100 Cost: 8384.662109\n",
      "Epoch   32/100 Cost: 8051.513184\n",
      "Epoch   33/100 Cost: 7731.657715\n",
      "Epoch   34/100 Cost: 7424.562500\n",
      "Epoch   35/100 Cost: 7129.714844\n",
      "Epoch   36/100 Cost: 6846.618652\n",
      "Epoch   37/100 Cost: 6574.804688\n",
      "Epoch   38/100 Cost: 6313.818848\n",
      "Epoch   39/100 Cost: 6063.225098\n",
      "Epoch   40/100 Cost: 5822.608398\n",
      "Epoch   41/100 Cost: 5591.568848\n",
      "Epoch   42/100 Cost: 5369.722168\n",
      "Epoch   43/100 Cost: 5156.700195\n",
      "Epoch   44/100 Cost: 4952.150879\n",
      "Epoch   45/100 Cost: 4755.734863\n",
      "Epoch   46/100 Cost: 4567.127930\n",
      "Epoch   47/100 Cost: 4386.015625\n",
      "Epoch   48/100 Cost: 4212.100586\n",
      "Epoch   49/100 Cost: 4045.094482\n",
      "Epoch   50/100 Cost: 3884.724121\n",
      "Epoch   51/100 Cost: 3730.721924\n",
      "Epoch   52/100 Cost: 3582.835449\n",
      "Epoch   53/100 Cost: 3440.821533\n",
      "Epoch   54/100 Cost: 3304.444580\n",
      "Epoch   55/100 Cost: 3173.481201\n",
      "Epoch   56/100 Cost: 3047.716064\n",
      "Epoch   57/100 Cost: 2926.941895\n",
      "Epoch   58/100 Cost: 2810.960205\n",
      "Epoch   59/100 Cost: 2699.579590\n",
      "Epoch   60/100 Cost: 2592.617676\n",
      "Epoch   61/100 Cost: 2489.898682\n",
      "Epoch   62/100 Cost: 2391.254395\n",
      "Epoch   63/100 Cost: 2296.521973\n",
      "Epoch   64/100 Cost: 2205.546875\n",
      "Epoch   65/100 Cost: 2118.178955\n",
      "Epoch   66/100 Cost: 2034.275757\n",
      "Epoch   67/100 Cost: 1953.698853\n",
      "Epoch   68/100 Cost: 1876.317139\n",
      "Epoch   69/100 Cost: 1802.002686\n",
      "Epoch   70/100 Cost: 1730.633789\n",
      "Epoch   71/100 Cost: 1662.094727\n",
      "Epoch   72/100 Cost: 1596.271973\n",
      "Epoch   73/100 Cost: 1533.057617\n",
      "Epoch   74/100 Cost: 1472.348999\n",
      "Epoch   75/100 Cost: 1414.046021\n",
      "Epoch   76/100 Cost: 1358.054321\n",
      "Epoch   77/100 Cost: 1304.280640\n",
      "Epoch   78/100 Cost: 1252.638306\n",
      "Epoch   79/100 Cost: 1203.041626\n",
      "Epoch   80/100 Cost: 1155.411255\n",
      "Epoch   81/100 Cost: 1109.667603\n",
      "Epoch   82/100 Cost: 1065.735596\n",
      "Epoch   83/100 Cost: 1023.544312\n",
      "Epoch   84/100 Cost: 983.025269\n",
      "Epoch   85/100 Cost: 944.111023\n",
      "Epoch   86/100 Cost: 906.738770\n",
      "Epoch   87/100 Cost: 870.846802\n",
      "Epoch   88/100 Cost: 836.376953\n",
      "Epoch   89/100 Cost: 803.271973\n",
      "Epoch   90/100 Cost: 771.478638\n",
      "Epoch   91/100 Cost: 740.945007\n",
      "Epoch   92/100 Cost: 711.620911\n",
      "Epoch   93/100 Cost: 683.458130\n",
      "Epoch   94/100 Cost: 656.410889\n",
      "Epoch   95/100 Cost: 630.435181\n",
      "Epoch   96/100 Cost: 605.487915\n",
      "Epoch   97/100 Cost: 581.528809\n",
      "Epoch   98/100 Cost: 558.518677\n",
      "Epoch   99/100 Cost: 536.420349\n",
      "Epoch  100/100 Cost: 515.196899\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, norm_x_train, y_train)\n",
    "\n",
    "# Epoch    1/100 Cost: 28484.753906\n",
    "# Epoch    2/100 Cost: 27337.447266\n",
    "# Epoch    3/100 Cost: 26237.316406\n",
    "# Epoch    4/100 Cost: 25182.328125\n",
    "# Epoch    5/100 Cost: 24170.564453\n",
    "# Epoch    6/100 Cost: 23200.175781\n",
    "# Epoch    7/100 Cost: 22269.408203\n",
    "# Epoch    8/100 Cost: 21376.582031\n",
    "# Epoch    9/100 Cost: 20520.101562\n",
    "# Epoch   10/100 Cost: 19698.441406\n",
    "# Epoch   11/100 Cost: 18910.128906\n",
    "# Epoch   12/100 Cost: 18153.785156\n",
    "# Epoch   13/100 Cost: 17428.066406\n",
    "# Epoch   14/100 Cost: 16731.705078\n",
    "# Epoch   15/100 Cost: 16063.479492\n",
    "# Epoch   16/100 Cost: 15422.226562\n",
    "# Epoch   17/100 Cost: 14806.833008\n",
    "# Epoch   18/100 Cost: 14216.231445\n",
    "# Epoch   19/100 Cost: 13649.401367\n",
    "# Epoch   20/100 Cost: 13105.369141\n",
    "# Epoch   21/100 Cost: 12583.198242\n",
    "# Epoch   22/100 Cost: 12081.996094\n",
    "# Epoch   23/100 Cost: 11600.904297\n",
    "# Epoch   24/100 Cost: 11139.104492\n",
    "# Epoch   96/100 Cost: 605.487915\n",
    "# Epoch   97/100 Cost: 581.528809\n",
    "# Epoch   98/100 Cost: 558.518677\n",
    "# Epoch   99/100 Cost: 536.420349\n",
    "# Epoch  100/100 Cost: 515.196899"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./image/2022-01-17-22-47-45.png)\n",
    "\n",
    "최적화를 하지 않으면 첫 column만 줄어들고 두 번째 column은 그대로 유지. 이미 작으니깐\n",
    "이를 해결하기 위해 normalization을 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 100\n",
    "\n",
    "mnist_train = dsets.MNIST(root=\"MNIST_data/\",\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "mnist_test = dsets.MNIST(root=\"MNIST_data/\",\n",
    "                          train=False,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "# 불러온 값을 사용하기 위해 dataloader를 사용한다.\n",
    "data_loader=torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=True) # batch_size에 안맞고 남는 data 버린다.\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1,28*28) # reshape image to 784, but Label should be onehotecoded\n",
    "        # 참고로 X는 (batch size, 1, 28, 28)이었는데 그 이후 (batch size, 784)가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.575375974\n",
      "Epoch: 0002 cost = 0.374016970\n",
      "Epoch: 0003 cost = 0.342421621\n",
      "Epoch: 0004 cost = 0.326022476\n",
      "Epoch: 0005 cost = 0.315575600\n",
      "Epoch: 0006 cost = 0.307909369\n",
      "Epoch: 0007 cost = 0.302397579\n",
      "Epoch: 0008 cost = 0.297522157\n",
      "Epoch: 0009 cost = 0.293116540\n",
      "Epoch: 0010 cost = 0.290487796\n",
      "Epoch: 0011 cost = 0.287488163\n",
      "Epoch: 0012 cost = 0.285288155\n",
      "Epoch: 0013 cost = 0.282853276\n",
      "Epoch: 0014 cost = 0.280928671\n",
      "Epoch: 0015 cost = 0.279262871\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(784,10,bias=True)\n",
    "training_epochs = 15\n",
    "batch_size = 128\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # F.cross_entropy는 정의 할 때 모든 걸 설정해야한다.\n",
    "optimizer = optim.SGD(linear.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    len_total_batch = len(data_loader)\n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / len_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.890999972820282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # test할 때 꼭 넣어주자. 오류가 줄어든다.\n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print(\"Accuracy : \", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label :  0\n",
      "Prediction :  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOOUlEQVR4nO3df5BddXnH8c+HzRIgkDYxJeyECArBFqUGWQIW7aBMGaBOE4axNTNVdNBoJRYsxWZwWijtTLGKxHYqNpSMsaUwIFJooRUmYtHyY7JgSoIBQ2kKSdYslKnBjuTXPv1jL50l7Pnezf29ed6vmZ1773nu2fPMmf3sufd+zz1fR4QAHPwO6XYDADqDsANJEHYgCcIOJEHYgSSmdXJjh3p6HKYZndwkkMqr+l/tjl2eqNZU2G2fJ+krkvok/U1EXFd6/mGaoTN8TjObBFDwWKytrDX8Mt52n6S/knS+pJMlLbV9cqO/D0B7NfOefZGkZyPiuYjYLek2SYtb0xaAVmsm7PMkvTDu8dbastexvcz2kO2hPdrVxOYANKOZsE/0IcAbzr2NiFURMRgRg/2a3sTmADSjmbBvlTR/3ONjJW1vrh0A7dJM2NdJWmD7LbYPlfQhSfe0pi0Ardbw0FtE7LW9XNK3NTb0tjoinmpZZwBaqqlx9oi4T9J9LeoFQBtxuiyQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJNDWLKw5+fW9/W7F+0Tf/tVhfeNjzlbXP/ui3iuvu/KeBYn3uXz5crOP1mgq77S2SXpG0T9LeiBhsRVMAWq8VR/b3RcRLLfg9ANqI9+xAEs2GPSTdb/tx28smeoLtZbaHbA/t0a4mNwegUc2+jD8rIrbbPlrSA7afjoiHxj8hIlZJWiVJMz07mtwegAY1dWSPiO212xFJd0la1IqmALRew2G3PcP2Ua/dl3SupI2tagxAazXzMn6upLtsv/Z7/j4i/qUlXaFj+mbNKtZP+btnivWPztxerP/yox+rrC1d8Hhx3StX3F6sv2Pe7xbrb13xSLGeTcNhj4jnJL2zhb0AaCOG3oAkCDuQBGEHkiDsQBKEHUiCr7gm99Jv/GKx/oW5Nxbr3/1Z+U/ouE+NVNYe6TuuuO7br393sb7+t28o1j/wvc9U1qbfu6647sGIIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4+0Gub86bivVv/skX6/yGI4vVy/7iU8X6MS82frnnt63oL9a/853ZxfrwR6ovg3b8vQ21NKVxZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBhnP8g9ffWJxfq8viOK9SuG31WsD3ytfDnoZqYA2vvC1mL92i9cXKz/4x9+qbJ26XuXF9c95Hs/KNanIo7sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+wHgb6TTqis/eDClXXWnl6sPv5Hp5XX3tW966/P/fYLxfoJf3x4ZW3vEX3FdQ9tqKPeVvfIbnu17RHbG8ctm237Aduba7flSb4BdN1kXsZ/XdJ5+y1bIWltRCyQtLb2GEAPqxv2iHhI0sv7LV4saU3t/hpJS1rbFoBWa/QDurkRMSxJtdujq55oe5ntIdtDe1R9TTAA7dX2T+MjYlVEDEbEYH+dD4MAtE+jYd9he0CSarfVU3UC6AmNhv0eSa99v/BiSXe3ph0A7VJ3nN32rZLOljTH9lZJV0u6TtLtti+R9LykD7azSZTNWfNiZe1Il986Ld7868X6jPXbivW9xWrv6ts92u0WOq5u2CNiaUXpnBb3AqCNOF0WSIKwA0kQdiAJwg4kQdiBJPiK6xSw6/zTi/Wb3vzVytq2feVTlPdeOadYj20bivWuivKFqndF9cDgfy4uf4n1xAcb6qincWQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ58Ctn9kd7E+TdWXRf7Ays8V1x1Y93BDPfWCvVvLX7+9bNv7OtTJ1MCRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJy9B/TNnFms/+lp/1Csf+7Hg5W1Y295trjuvmK1t007Zm6xftUxd1bWhp59Z6vb6Xkc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZe8Az155crF8047vF+p+tPLOydvSOqft99br6+4vlN087orI2Z8PPWt1Nz6t7ZLe92vaI7Y3jll1je5vt9bWfC9rbJoBmTeZl/NclnTfB8hsiYmHt577WtgWg1eqGPSIekvRyB3oB0EbNfEC33PaTtZf5s6qeZHuZ7SHbQ3tUnncMQPs0GvYbJZ0gaaGkYUnXVz0xIlZFxGBEDPZreoObA9CshsIeETsiYl9EjEq6SdKi1rYFoNUaCrvtgXEPL5S0seq5AHpD3XF227dKOlvSHNtbJV0t6WzbCyWFpC2SPtm+Fg9+v3fuvcX6ht17ivWBOzZX1qby99Wb9d+j1WPp035S/vxotNXN9IC6YY+IpRMsvrkNvQBoI06XBZIg7EAShB1IgrADSRB2IAm+4toBo+9ZWKwv+7ny4MYp//bxYv24FzccaEtTgqeV/zxHbqz+CqskXfvj91fWRtf/sKGepjKO7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsHfDcRYcV64fIxXr/0FGtbGfK8OGHF+uPnnpbsb7o2ksra3P0SEM9TWUc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZO6E8jF7XzC0H44WN63v+M6cU65/etrdYn/PX+cbSSziyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLNPAT//8AvFenm0uXftPu/0Yv3B3/lisf6xRRfV2UL1lM0Z1T2y255v+0Hbm2w/Zfuy2vLZth+wvbl2O6v97QJo1GRexu+VdEVE/JKkMyVdavtkSSskrY2IBZLW1h4D6FF1wx4RwxHxRO3+K5I2SZonabGkNbWnrZG0pE09AmiBA/qAzvbxkk6V9JikuRExLI39Q5B0dMU6y2wP2R7ao11NtgugUZMOu+0jJd0p6fKI2DnZ9SJiVUQMRsRgv6Y30iOAFphU2G33ayzot0TEt2qLd9geqNUHJI20p0UArVB36M22Jd0saVNEfHlc6R5JF0u6rnZ7d1s6xJQ2+t5TK2t33LSyuO4Zd1xRrJ84/GgjLaU1mXH2syR9WNIG2+try67SWMhvt32JpOclfbAtHQJoibphj4jvq/ryC+e0th0A7cLpskAShB1IgrADSRB2IAnCDiTBV1w74KTV/1OsP7Vkd7G+8/Rji/Ujtm470JZaZmT5rxTrN3z2a5W1Mx9aXlz3xN9f11BPmBhHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2Dhh98uli/cK7Li/W/3nl9cX6RSdeWVk76vnydM873l0sa8bxPynWP31S+TIGn1/xicraCXcOlTc+uq9cxwHhyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTgiOraxmZ4dZ5gL0u7P08qnO4x8vDy18eFLdlTWTptTnu75iZfmF+s77z+mWJ/31SeK9dFXXy3W0VqPxVrtjJcnvBo0R3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLuOLvt+ZK+IekYSaOSVkXEV2xfI+kTkl6sPfWqiLiv9LsYZwfaqzTOPpmLV+yVdEVEPGH7KEmP236gVrshIr7UqkYBtM9k5mcfljRcu/+K7U2S5rW7MQCtdUDv2W0fL+lUSY/VFi23/aTt1bZnVayzzPaQ7aE92tVctwAaNumw2z5S0p2SLo+InZJulHSCpIUaO/JPeKG0iFgVEYMRMdiv6c13DKAhkwq77X6NBf2WiPiWJEXEjojYFxGjkm6StKh9bQJoVt2w27akmyVtiogvj1s+MO5pF0ra2Pr2ALTKZD6NP0vShyVtsL2+tuwqSUttL5QUkrZI+mQb+gPQIpP5NP77kiYatyuOqQPoLZxBByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKKjUzbbflHSf41bNEfSSx1r4MD0am+92pdEb41qZW/HRcQvTFToaNjfsHF7KCIGu9ZAQa/21qt9SfTWqE71xst4IAnCDiTR7bCv6vL2S3q1t17tS6K3RnWkt66+ZwfQOd0+sgPoEMIOJNGVsNs+z/Yztp+1vaIbPVSxvcX2BtvrbQ91uZfVtkdsbxy3bLbtB2xvrt1OOMdel3q7xva22r5bb/uCLvU23/aDtjfZfsr2ZbXlXd13hb46st86/p7ddp+kH0n6NUlbJa2TtDQiftjRRirY3iJpMCK6fgKG7V+V9FNJ34iId9SW/bmklyPiuto/ylkR8Qc90ts1kn7a7Wm8a7MVDYyfZlzSEkkfVRf3XaGv31QH9ls3juyLJD0bEc9FxG5Jt0la3IU+el5EPCTp5f0WL5a0pnZ/jcb+WDquoreeEBHDEfFE7f4rkl6bZryr+67QV0d0I+zzJL0w7vFW9dZ87yHpftuP217W7WYmMDcihqWxPx5JR3e5n/3Vnca7k/abZrxn9l0j0583qxthn2gqqV4a/zsrIt4l6XxJl9ZermJyJjWNd6dMMM14T2h0+vNmdSPsWyXNH/f4WEnbu9DHhCJie+12RNJd6r2pqHe8NoNu7Xaky/38v16axnuiacbVA/uum9OfdyPs6yQtsP0W24dK+pCke7rQxxvYnlH74ES2Z0g6V703FfU9ki6u3b9Y0t1d7OV1emUa76ppxtXlfdf16c8jouM/ki7Q2Cfy/yHp893ooaKvt0r699rPU93uTdKtGntZt0djr4gukfQmSWslba7dzu6h3v5W0gZJT2osWANd6u09Gntr+KSk9bWfC7q97wp9dWS/cboskARn0AFJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEv8HOMwmOZ9lxFUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "r = random.randint(0, len(mnist_test) - 1)\n",
    "x_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float()\n",
    "Y_single_data = mnist_test.test_labels[r:r+1]\n",
    "\n",
    "print(\"Label : \", Y_single_data.item())\n",
    "single_prediction = linear(x_single_data)\n",
    "print(\"Prediction : \", torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "plt.imshow(mnist_test.test_data[r:r+1].view(28,28))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
