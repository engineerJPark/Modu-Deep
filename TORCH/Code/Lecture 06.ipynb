{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classification (=Multinomial Classfication)\n",
    "\n",
    "여러개의 Classification에 관한 Prob이다.\n",
    "\n",
    "binary classification, 즉, sigmoid를 여러번 해서 구할 수도 있기는 하다.\n",
    "\n",
    "![](./image/2022-01-13-14-17-35.png)\n",
    "\n",
    "![](./image/2022-01-13-14-17-47.png)\n",
    "\n",
    "----\n",
    "\n",
    "**중요한 것은, softmax가 단순히 classification만이 목적이 아니라, 확률 분포를 근사한다는 사실에 주목해야한다.**\n",
    "\n",
    "classification은 단순히 이를 응용한 것에 불과하다.\n",
    "\n",
    "----\n",
    "\n",
    "하지만 위의 방식대로 한다면 각 sample 별로 연산을 따로 해야하는 문제가 있다. 이는 연산양을 늘린다.\n",
    "\n",
    "![](./image/2022-01-13-17-32-01.png)\n",
    "\n",
    "따라서 위 그림처럼 하나의 행렬을 만들어서 연산을 할 것이다.\n",
    "\n",
    "이제 이것 대신에, softmax에 넣어서 확률로 바꿔준다.\n",
    "\n",
    "![](./image/2022-01-13-17-50-19.png)\n",
    "![](./image/2022-01-13-17-52-03.png)\n",
    "\n",
    "이렇게 행렬곱 결과로 나온 것을 softmax를 거쳐서, 각 class에 대한 확률을 내뱉는 것이다. 확률이므로 이 값들의 합은 1이 된다.\n",
    "\n",
    "![](./image/2022-01-13-17-52-14.png)\n",
    "\n",
    "softmax를 거친 결과 나온 확률을 one hot incoding을 통해서 [1,0,0]과 같은 형태로 만들어준다. 어떤 class인가를 확실하게 언급하는 것이다.\n",
    "\n",
    "# Cost Function : Cross Entropy\n",
    "\n",
    "![](./image/2022-01-16-19-40-44.png)\n",
    "\n",
    "Cross Entropy는 두 확률이 얼마나 비슷한지에 대한 지표가 된다.\n",
    "\n",
    "![](./image/2022-01-14-15-13-15-image.png)\n",
    "\n",
    "확률 분포 P에서 $x$를 샘플링하고, 그 $x$에 대해서 Q를 구한다.\n",
    "\n",
    "![](./image/2022-01-14-15-25-50-image.png)\n",
    "\n",
    "----\n",
    "\n",
    "![](./image/2022-01-13-17-53-46.png)\n",
    "\n",
    "왼쪽이 예측값, 오른쪽이 실제값이 된다. 그리고 중앙의 식이 그 차이인 cross-entropy이다.\n",
    "\n",
    "![](./image/2022-01-13-17-55-06.png)\n",
    "\n",
    "이렇게 정의되고, $L_i$는 예측값, $-\\log y_i$는 실제값에 log를 씌운 것이다.\n",
    "\n",
    "다음 그림은 실제값이 A인 일 때, 예측을 A로 했을 때와 B로 했을 때이다.\n",
    "![](./image/2022-01-13-17-57-04.png)\n",
    "\n",
    "다음 그림은 실제값이 B인 일 때, 예측을 A로 했을 때와 B로 했을 때이다.\n",
    "![](./image/2022-01-13-17-57-14.png)\n",
    "\n",
    "즉 예측과 실제가 맞으면 cost가 0이되고, 틀리면 무한이 되므로, 적합한 cost function임을 알 수 있다.\n",
    "\n",
    "----\n",
    "\n",
    "# Logistic Cost와 Cross Entropy는 그 의미가 같다.\n",
    "\n",
    "![](./image/2022-01-13-18-00-08.png)\n",
    "\n",
    "두 함수가 같은 이유는 두개의 예측 및 결과만 있기에  $-\\sigma(Li * log(Si) = -(L1*log(S1))-(L2*log(S2))$ 입니다. \n",
    "\n",
    "실제 값 $L1,L2$은 1과 0, 그리고 서로 반대의 값을 지닐수밖에 없기 때문에 $L2 = 1-L1$ 일 수밖에 없습니다. (0 또는 1, 1또는 0) \n",
    "\n",
    "S1, S2은 예측값이기 때문에 1,0 만 나오는 것은 아니지만 둘의 합은 1이 될 수밖에 없습니다. (0.3, 0.7 등 그러나 어쨌든 $1-L0 = L1$)\n",
    "\n",
    "따라서 $-L1*log(S1)-(1-L1)*log(1-S1)$ 가 됩니다. \n",
    "$L1 = y, S1 = H(x)$ 로 바꾸면 $-y*log(H(x))-(1-y)*log(1-H(x))$가 되는 것입니다.\n",
    "\n",
    "----  \n",
    "\n",
    "# 다시 Cost Function을 보자.\n",
    "\n",
    "![](./image/2022-01-14-12-34-05.png)\n",
    "\n",
    "이번에도 Cost function은 실제값과 예측값 사이의 distance로 정의해주면 된다. 뒤에서 코드로 보자.\n",
    "\n",
    "![](./image/2022-01-14-12-34-40.png)\n",
    "\n",
    "Optimizing은 이 Cost Function을 편미분해서 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distribution\n",
    "\n",
    "### PMF : discrete\n",
    "\n",
    "point의 높이가 확률을 의미\n",
    "\n",
    "![](./image/2022-01-14-12-48-54.png)\n",
    "\n",
    "### PDF : continuous\n",
    "\n",
    "not point, 면적이 확률을 의미\n",
    "\n",
    "![](./image/2022-01-14-12-48-22.png)\n",
    "\n",
    "### softmax 구현하기\n",
    "\n",
    "softmax를 거쳐서 나오는 확률은 다음과 같다.\n",
    "\n",
    "![](./image/2022-01-14-12-55-40.png)\n",
    "\n",
    "----\n",
    "\n",
    "이런 경우의 텐서에서 argmax를 구하면 다음과 같다.\n",
    "\n",
    "![](./image/2022-01-14-14-55-38.png)\n",
    "\n",
    "참고로 one hot incoding 이후, softmax를 거쳐 나온 것([0.85,0.01,0.02,0.05,0.07])이나, 원래 data label에서 one hot incoding을 거친 결과물([0,0,1,0,0,0])이나 어차피 똑같이 '확률'을 의미한다. 단순히 data label에서 얻은 확률은 '이것이 100% 맞다.'는 의미일 뿐.\n",
    "\n",
    "하나의 학습에서 모든 class의 예측값으로 나온 softmax의 합은 1이 된다.\n",
    "\n",
    "모델의 맨 마지막에 softmax를 둔다. 이후, maximum likelihood estimation을 통해 훈련한다.\n",
    "\n",
    "\n",
    "\n",
    "### Cross Entropy 구현하기\n",
    "\n",
    "cross entropy는 그 식이 다음과 같다.\n",
    "\n",
    "![](./image/2022-01-16-19-36-16.png)\n",
    "![](./image/2022-01-16-19-37-03.png)\n",
    "\n",
    "$y$는 실제값이므로 $P(x)$이고, $\\hat y$는 예측값 $Q(x)$를 의미한다. $Q(x)$는 가끔 $P_\\theta(x)$라고도 한다. \n",
    "\n",
    "one hot incoding은 `.scatter()`나 `.scatter()_`(이경우 inclass 연산)를 사용한다.\n",
    "\n",
    "이에대한 참고문서는\n",
    "\n",
    "[torch.Tensor.scatter_ — PyTorch 1.10.1 documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html)\n",
    "\n",
    "[torch.Tensor.scatter_ — PyTorch 1.10.1 documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html)\n",
    "\n",
    "참고로 후자의 method를 써야 메모리가 새로 할당되지 않는다. inclass method는 메모리 할당이 없다.\n",
    "\n",
    "\n",
    "\n",
    "Cost가 되는 cross entropy를 code로 옮기면 다음과 같다.\n",
    "\n",
    "![](./image/2022-01-16-19-37-37.png)\n",
    "\n",
    "\n",
    "\n",
    "high level 표현은 github에서 코드를 직접 확인하자.\n",
    "\n",
    "참고로 functional 패키지의 cross_entropy는 이미 softmax가 들어있다. 이에 주의.\n",
    "\n",
    "\n",
    "\n",
    "구현할 때 다음과 같은 사항을 잊지말자.\n",
    "\n",
    "Binary Classification이면 : \n",
    "\n",
    "- sigmoid\n",
    "\n",
    "- Binary Cross Entropy\n",
    "\n",
    "Multinomial Classification이면 : \n",
    "\n",
    "- softmax\n",
    "\n",
    "- Cross Entropy\n",
    "\n",
    "\n",
    "\n",
    "이제 구현을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "# softmax 구현\n",
    "z = torch.FloatTensor([1,2,3])\n",
    "\n",
    "# torch 내장 softmax\n",
    "hypothesis = F.softmax(z, dim = 0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2974, 0.2244, 0.1595, 0.1420, 0.1767],\n",
      "        [0.1509, 0.1595, 0.1917, 0.2522, 0.2457],\n",
      "        [0.1002, 0.2226, 0.1878, 0.2624, 0.2271]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy 구현\n",
    "z = torch.rand(3,5,requires_grad=True)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)\n",
    "y = torch.randint(5,(3,)).long() # data type long으로 변경\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6471, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# cross entropy\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean() # 하나의 지표로 나타내기 위해서 mean으로 처리해버린다.\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6471, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다시 풀어서 정리하면 다음과 같다.\n",
    "\n",
    "# Cross Entropy\n",
    "# low level에서의 cross entorpy와 loss\n",
    "torch.log(F.softmax(z, dim=1))\n",
    "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()\n",
    "\n",
    "# hight level에서의 cross entropy와 loss \n",
    "F.log_softmax(z, dim=1)\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y) # negative log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대신에 다음을 사용할수도 있다.\n",
    "\n",
    "F.cross_entropy(z, y)\n",
    "\n",
    "F.log_softmax(z, dim=1)\n",
    "\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)\n",
    "\n",
    "첫번째 인수는 log probability이고, 두번째 인수는 target이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제는 cross entropy로 학습을 하는 과정을 거쳐보자..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 1.036919\n",
      "Epoch  200/1000 Cost: 1.010905\n",
      "Epoch  300/1000 Cost: 0.991541\n",
      "Epoch  400/1000 Cost: 0.974942\n",
      "Epoch  500/1000 Cost: 0.959928\n",
      "Epoch  600/1000 Cost: 0.946149\n",
      "Epoch  700/1000 Cost: 0.933492\n",
      "Epoch  800/1000 Cost: 0.921898\n",
      "Epoch  900/1000 Cost: 0.911302\n",
      "Epoch 1000/1000 Cost: 0.901625\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1,2,1,1],\n",
    "                            [2,1,3,2],\n",
    "                            [3,1,3,4],\n",
    "                            [4,1,5,5],\n",
    "                            [1,7,5,5],\n",
    "                            [1,2,5,6],\n",
    "                            [1,6,6,6],\n",
    "                            [1,7,7,7]])\n",
    "y_train = torch.LongTensor([2,2,2,1,1,1,0,0])\n",
    "\n",
    "W = torch.zeros((4,3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "optimizer = optim.SGD([W,b], lr=0.01)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs + 1):\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "    y_one_hot = torch.zeros_like(hypothesis)\n",
    "    y_one_hot.scatter_(1, y_train.unsqueeze(1), 1) # y_train이 주는 index에 따라서 1을 뿌린다는 뜻이다.\n",
    "    cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, cost.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.098612\n",
      "Epoch  100/1000 Cost: 0.850816\n",
      "Epoch  200/1000 Cost: 0.784908\n",
      "Epoch  300/1000 Cost: 0.744590\n",
      "Epoch  400/1000 Cost: 0.714646\n",
      "Epoch  500/1000 Cost: 0.690688\n",
      "Epoch  600/1000 Cost: 0.670780\n",
      "Epoch  700/1000 Cost: 0.653828\n",
      "Epoch  800/1000 Cost: 0.639125\n",
      "Epoch  900/1000 Cost: 0.626180\n",
      "Epoch 1000/1000 Cost: 0.614641\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1,2,1,1],\n",
    "                            [2,1,3,2],\n",
    "                            [3,1,3,4],\n",
    "                            [4,1,5,5],\n",
    "                            [1,7,5,5],\n",
    "                            [1,2,5,6],\n",
    "                            [1,6,6,6],\n",
    "                            [1,7,7,7]])\n",
    "y_train = torch.LongTensor([2,2,2,1,1,1,0,0])\n",
    "\n",
    "W = torch.zeros((4,3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "optimizer = optim.SGD([W,b], lr=0.01)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs + 1):\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(hypothesis, y_train) # onehot incoding을 안해도된다.\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, cost.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 1.777960\n",
      "Epoch  100/1000 Cost: 1.004641\n",
      "Epoch  200/1000 Cost: 0.827739\n",
      "Epoch  300/1000 Cost: 0.749356\n",
      "Epoch  400/1000 Cost: 0.697707\n",
      "Epoch  500/1000 Cost: 0.659141\n",
      "Epoch  600/1000 Cost: 0.628730\n",
      "Epoch  700/1000 Cost: 0.603923\n",
      "Epoch  800/1000 Cost: 0.583172\n",
      "Epoch  900/1000 Cost: 0.565461\n",
      "Epoch 1000/1000 Cost: 0.550089\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1,2,1,1],\n",
    "                            [2,1,3,2],\n",
    "                            [3,1,3,4],\n",
    "                            [4,1,5,5],\n",
    "                            [1,7,5,5],\n",
    "                            [1,2,5,6],\n",
    "                            [1,6,6,6],\n",
    "                            [1,7,7,7]])\n",
    "y_train = torch.LongTensor([2,2,2,1,1,1,0,0])\n",
    "\n",
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3) # feature의 개수만 받는다. input, output 모두. 그리고 W,b를 내장으로 가지고 있다.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)    \n",
    "\n",
    "model = SoftmaxClassifierModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs + 1):\n",
    "    hypothesis = model(x_train)\n",
    "    cost = F.cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, epochs, cost.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Lienar에 대해선 https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
