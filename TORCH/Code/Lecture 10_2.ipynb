{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "trans = transforms.Compose([ # transform을 여러번 할 거면 이렇게 묶어서 할 수 있다.\n",
        "    transforms.Resize((64,128))\n",
        "])\n",
        "\n",
        "# 제목에 _ 뒤에 있는 숫자가 label. \n",
        "# root 이하의 폴더를 recursive하게 조사한다.\n",
        "train_data = torchvision.datasets.ImageFolder(root='./custom_data/origin_data',\n",
        "                                              transform=trans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "for num, value in enumerate(train_data):\n",
        "    data, label = value\n",
        "    print(num, data, label)\n",
        "    \n",
        "    if(label == 0):\n",
        "        data.save('custom_data/train_data/gray/%d_%d.jpeg'%(num, label))\n",
        "    else:\n",
        "        data.save('custom_data/train_data/red/%d_%d.jpeg'%(num, label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usage of ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "trans = transforms.Compose([\n",
        "    transforms.ToTensor() # PIL to Tensor\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(root='./custom_data/train_data',\n",
        "                                              transform=trans)\n",
        "\n",
        "data_loader = DataLoader(dataset=train_data, # ImageFolder를 이용해서 가져왔다.\n",
        "                         batch_size=32,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True,\n",
        "                         num_workers=2) # 멀티프로세싱 개수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[nan, nan]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.FC = nn.Sequential( # need flatten before input\n",
        "            nn.Linear(16*13*29, 120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120,2)\n",
        "        )\n",
        "        # sigmoid after\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        out = self.FC(out)\n",
        "        return out\n",
        "\n",
        "#testing \n",
        "net = CNN().to(device)\n",
        "test_input = (torch.Tensor(1,3,64,128)).to(device)\n",
        "test_out = model(test_input)\n",
        "test_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
        "loss_func = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch:2] cost = 0.5089213252067566\n",
            "[Epoch:3] cost = 0.05813751369714737\n",
            "[Epoch:4] cost = 0.0027425051666796207\n",
            "[Epoch:5] cost = 0.0002715228183660656\n",
            "[Epoch:6] cost = 0.00014487621956504881\n",
            "[Epoch:7] cost = 9.05981651158072e-05\n",
            "[Epoch:8] cost = 7.206144073279575e-05\n",
            "[Epoch:9] cost = 6.118701276136562e-05\n",
            "[Epoch:10] cost = 5.15760802954901e-05\n",
            "[Epoch:11] cost = 5.329633131623268e-05\n",
            "[Epoch:12] cost = 3.35050790454261e-05\n",
            "[Epoch:13] cost = 2.810091791616287e-05\n",
            "[Epoch:14] cost = 2.005798160098493e-05\n",
            "[Epoch:15] cost = 1.5580319086438976e-05\n",
            "[Epoch:16] cost = 1.2331304787949193e-05\n",
            "[Epoch:17] cost = 9.922556273522787e-06\n",
            "[Epoch:18] cost = 8.26423547550803e-06\n",
            "[Epoch:19] cost = 7.0877536018087994e-06\n",
            "[Epoch:20] cost = 6.202488748385804e-06\n",
            "[Epoch:21] cost = 5.4202027968131006e-06\n"
          ]
        }
      ],
      "source": [
        "total_batch = len(data_loader)\n",
        "\n",
        "training_epochs = 20\n",
        "for epoch in range(1, training_epochs + 1):\n",
        "    avg_cost = 0\n",
        "    for num, data in enumerate(data_loader):\n",
        "        imgs, label = data\n",
        "        imgs = imgs.to(device)\n",
        "        label = label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = net(imgs)\n",
        "        loss = loss_func(hypothesis, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_cost += loss / total_batch\n",
        "    print('[Epoch:{}] cost = {}'.format(epoch+1, avg_cost))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model 저장 및 load\n",
        "\n",
        "torch.save(model.state_dict(), \"./model/model.pth\")\n",
        "new_model = CNN().to(device) # 모델의 구성은 동일해야한다.\n",
        "new_model.load_state_dict(torch.load('./model/model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1)) Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "tensor([-0.0929,  0.0014, -0.0193, -0.0239,  0.0900],\n",
            "       grad_fn=<SelectBackward0>) tensor([ 0.0595, -0.0510, -0.0224,  0.0542, -0.1087],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False]],\n",
              "\n",
              "        [[False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False]],\n",
              "\n",
              "        [[False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False],\n",
              "         [False, False, False, False, False]]])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 저장된 것 비교\n",
        "print(net.layer1[0], new_model.layer1[0])\n",
        "print(net.layer1[0].weight[0,0,0], new_model.layer1[0].weight[0,0,0])\n",
        "\n",
        "net.layer1[0].weight[0] == new_model.layer1[0].weight[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "trans = transforms.Compose([\n",
        "    transforms.Resize((64,128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "test_data = torchvision.datasets.ImageFolder(\n",
        "    root='./custom_data/test_data',\n",
        "    transform=trans\n",
        "    )\n",
        "test_set = DataLoader(dataset=test_data, batch_size=len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    for num, data in enumerate(test_set):\n",
        "        imgs, label = data\n",
        "        imgs = imgs.to(device)\n",
        "        label = label.to(device)\n",
        "        \n",
        "        prediction = net(imgs)\n",
        "        accuracy = (torch.argmax(prediction, 1) == label).float().mean()\n",
        "    print(accuracy.item())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Lecture 10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
