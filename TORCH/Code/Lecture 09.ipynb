{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron and MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import random\n",
    "\n",
    "batch_size = 128\n",
    "training_epochs = 100\n",
    "\n",
    "mnist_train = dsets.MNIST(root=\"MNIST_data/\",\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "mnist_test = dsets.MNIST(root=\"MNIST_data/\",\n",
    "                          train=False,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "# 불러온 값을 batch로 맞추기 위해 dataloader를 사용한다.\n",
    "data_loader=torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=True) # batch_size에 안맞고 남는 data 버린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "linear = nn.Linear(784,10,bias=True)\n",
    "nn.init.normal_(linear.weight) # normal distribution으로 weight initialization\n",
    "\n",
    "optimizer = optim.Adam(linear.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost =  6.000259876\n",
      "Epoch:  0002 cost =  1.971456528\n",
      "Epoch:  0003 cost =  1.233181715\n",
      "Epoch:  0004 cost =  0.952599049\n",
      "Epoch:  0005 cost =  0.802862823\n",
      "Epoch:  0006 cost =  0.708308518\n",
      "Epoch:  0007 cost =  0.641792655\n",
      "Epoch:  0008 cost =  0.592713892\n",
      "Epoch:  0009 cost =  0.554369390\n",
      "Epoch:  0010 cost =  0.523402810\n",
      "Epoch:  0011 cost =  0.498058051\n",
      "Epoch:  0012 cost =  0.475769252\n",
      "Epoch:  0013 cost =  0.457886189\n",
      "Epoch:  0014 cost =  0.441623956\n",
      "Epoch:  0015 cost =  0.428177416\n"
     ]
    }
   ],
   "source": [
    "# single Layer\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1,28*28) # reshape image to 784, but Label should be onehotecoded\n",
    "        # 참고로 X는 (batch size, 1, 28, 28)이었는데 그 이후 (batch size, 784)가 된다.    \n",
    "        predict = linear(X)\n",
    "        cost = F.cross_entropy(predict, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print(\"Epoch: \", \"%04d\" % (epoch+1), \"cost = \", \"{:.9f}\".format(avg_cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost =  0.415787548\n",
      "Epoch:  0002 cost =  0.404293984\n",
      "Epoch:  0003 cost =  0.393926442\n",
      "Epoch:  0004 cost =  0.384406596\n",
      "Epoch:  0005 cost =  0.376875848\n",
      "Epoch:  0006 cost =  0.369347125\n",
      "Epoch:  0007 cost =  0.362255245\n",
      "Epoch:  0008 cost =  0.355386287\n",
      "Epoch:  0009 cost =  0.349586934\n",
      "Epoch:  0010 cost =  0.344415039\n",
      "Epoch:  0011 cost =  0.339549154\n",
      "Epoch:  0012 cost =  0.334672987\n",
      "Epoch:  0013 cost =  0.330124974\n",
      "Epoch:  0014 cost =  0.326059937\n",
      "Epoch:  0015 cost =  0.322006494\n"
     ]
    }
   ],
   "source": [
    "# Multi Layer\n",
    "\n",
    "# Linear에 가중치 초기화 method가 따로 없어서, 이렇게 선언해야한다.\n",
    "linear1 = nn.Linear(784,256,bias=True)\n",
    "linear2 = nn.Linear(256,256,bias=True)\n",
    "linear3 = nn.Linear(256,10,bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "torch.nn.init.normal_(linear1.weight)\n",
    "torch.nn.init.normal_(linear2.weight)\n",
    "torch.nn.init.normal_(linear3.weight)\n",
    "\n",
    "model = nn.Sequential(linear, relu,\n",
    "                      linear2, relu,\n",
    "                      linear3)\n",
    "\n",
    "optimizer = optim.Adam(linear.parameters(), lr=learning_rate)\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1,28*28) \n",
    "        predict = linear(X)\n",
    "        cost = F.cross_entropy(predict, Y) # 이미 softmax가 있다.\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print(\"Epoch: \", \"%04d\" % (epoch+1), \"cost = \", \"{:.9f}\".format(avg_cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8888999819755554\n",
      "Label:  9\n",
      "Prediction:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float() \n",
    "    Y_test = mnist_test.test_labels \n",
    "\n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float() \n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1] \n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier_Uniform distribution\n",
    "\n",
    "# def xavier_uniform_(tensor, gain=1):\n",
    "#     fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n",
    "#     std = gain * math.sqrt(2. / (fan_in + fan_out))\n",
    "#     a = math.sqrt(3.) * std\n",
    "#     with torch.no_grad():\n",
    "#         return tensor.uniform_(-a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost =  0.318682969\n",
      "Epoch:  0002 cost =  0.315017045\n",
      "Epoch:  0003 cost =  0.311255425\n",
      "Epoch:  0004 cost =  0.308744848\n",
      "Epoch:  0005 cost =  0.305688769\n",
      "Epoch:  0006 cost =  0.303019404\n",
      "Epoch:  0007 cost =  0.299978286\n",
      "Epoch:  0008 cost =  0.297845423\n",
      "Epoch:  0009 cost =  0.294370979\n",
      "Epoch:  0010 cost =  0.293242365\n",
      "Epoch:  0011 cost =  0.290827185\n",
      "Epoch:  0012 cost =  0.288658142\n",
      "Epoch:  0013 cost =  0.286851197\n",
      "Epoch:  0014 cost =  0.284332752\n",
      "Epoch:  0015 cost =  0.282683849\n"
     ]
    }
   ],
   "source": [
    "linear1 = nn.Linear(784,256,bias=True)\n",
    "linear2 = nn.Linear(256,256,bias=True)\n",
    "linear3 = nn.Linear(256,10,bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "\n",
    "model = nn.Sequential(linear, relu,\n",
    "                      linear2, relu,\n",
    "                      linear3)\n",
    "\n",
    "optimizer = optim.Adam(linear.parameters(), lr=learning_rate)\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1,28*28) \n",
    "        predict = linear(X)\n",
    "        cost = F.cross_entropy(predict, Y) # 이미 softmax가 있다.\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print(\"Epoch: \", \"%04d\" % (epoch+1), \"cost = \", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost =  0.281634301\n",
      "Epoch:  0002 cost =  0.279691398\n",
      "Epoch:  0003 cost =  0.277927905\n",
      "Epoch:  0004 cost =  0.276562065\n",
      "Epoch:  0005 cost =  0.274940699\n",
      "Epoch:  0006 cost =  0.273482412\n",
      "Epoch:  0007 cost =  0.272297025\n",
      "Epoch:  0008 cost =  0.270939738\n",
      "Epoch:  0009 cost =  0.269048482\n",
      "Epoch:  0010 cost =  0.267948955\n",
      "Epoch:  0011 cost =  0.267306060\n",
      "Epoch:  0012 cost =  0.265866101\n",
      "Epoch:  0013 cost =  0.264946967\n",
      "Epoch:  0014 cost =  0.263636112\n",
      "Epoch:  0015 cost =  0.262503535\n"
     ]
    }
   ],
   "source": [
    "# 더 deep한 Network를 형성한다.\n",
    "\n",
    "linear1 = nn.Linear(784,512,bias=True)\n",
    "linear2 = nn.Linear(512,512,bias=True)\n",
    "linear3 = nn.Linear(512,512,bias=True)\n",
    "linear4 = nn.Linear(512,512,bias=True)\n",
    "linear5 = nn.Linear(512,10,bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)\n",
    "\n",
    "model = nn.Sequential(linear, relu,\n",
    "                      linear2, relu,\n",
    "                      linear3, relu,\n",
    "                      linear4, relu,\n",
    "                      linear5)\n",
    "\n",
    "optimizer = optim.Adam(linear.parameters(), lr=learning_rate)\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1,28*28) \n",
    "        predict = linear(X)\n",
    "        cost = F.cross_entropy(predict, Y) # 이미 softmax가 있다.\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print(\"Epoch: \", \"%04d\" % (epoch+1), \"cost = \", \"{:.9f}\".format(avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost =  0.261538982\n",
      "Epoch:  0002 cost =  0.261063308\n",
      "Epoch:  0003 cost =  0.259867907\n",
      "Epoch:  0004 cost =  0.258937359\n",
      "Epoch:  0005 cost =  0.257843316\n",
      "Epoch:  0006 cost =  0.257619917\n",
      "Epoch:  0007 cost =  0.256563157\n",
      "Epoch:  0008 cost =  0.255656570\n",
      "Epoch:  0009 cost =  0.254655898\n",
      "Epoch:  0010 cost =  0.253812224\n",
      "Epoch:  0011 cost =  0.253492236\n",
      "Epoch:  0012 cost =  0.252786100\n",
      "Epoch:  0013 cost =  0.252092630\n",
      "Epoch:  0014 cost =  0.251044303\n",
      "Epoch:  0015 cost =  0.250247121\n"
     ]
    }
   ],
   "source": [
    "# dropout을 추가한다.\n",
    "\n",
    "linear1 = nn.Linear(784,512,bias=True)\n",
    "linear2 = nn.Linear(512,512,bias=True)\n",
    "linear3 = nn.Linear(512,512,bias=True)\n",
    "linear4 = nn.Linear(512,512,bias=True)\n",
    "linear5 = nn.Linear(512,10,bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n",
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)\n",
    "\n",
    "model = nn.Sequential(linear, relu, dropout,\n",
    "                      linear2, relu, dropout,\n",
    "                      linear3, relu, dropout,\n",
    "                      linear4, relu, dropout,\n",
    "                      linear5)\n",
    "\n",
    "optimizer = optim.Adam(linear.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train() # dropout 적용\n",
    "total_batch = len(data_loader)\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for X, Y in data_loader:\n",
    "        X = X.view(-1,28*28) \n",
    "        predict = linear(X)\n",
    "        cost = F.cross_entropy(predict, Y) # 이미 softmax가 있다.\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print(\"Epoch: \", \"%04d\" % (epoch+1), \"cost = \", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "# model.train()과 model.eval()을 적극 사용해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.873199999332428\n",
      "Label:  6\n",
      "Prediction:  6\n"
     ]
    }
   ],
   "source": [
    "# Test the model using test sets\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float() \n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1] \n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(root=\"MNIST_data/\",\n",
    "                                         train=True,\n",
    "                                         transform=torchvision.transforms.ToTensor(),\n",
    "                                         download=True)\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(root=\"MNIST_data/\",\n",
    "                                         train=False,\n",
    "                                         transform=torchvision.transforms.ToTensor(),\n",
    "                                         download=True)\n",
    "\n",
    "batch_size = 100\n",
    "training_epochs = 10\n",
    "learning_rate = 0.01\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "\n",
    "class Bn_Model(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear1 = torch.nn.Linear(784,32,bias=True)\n",
    "    self.linear2 = torch.nn.Linear(32,32,bias=True)\n",
    "    self.linear3 = torch.nn.Linear(32,10,bias=True)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.bn1 = torch.nn.BatchNorm1d(32)\n",
    "    self.bn2 = torch.nn.BatchNorm1d(32)\n",
    "    self.model = torch.nn.Sequential(\n",
    "        self.linear1, self.bn1, self.relu,\n",
    "        self.linear2, self.bn2, self.relu,\n",
    "        self.linear3\n",
    "    ).to(device)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    return self.model(X)\n",
    "\n",
    "\n",
    "bn_model = Bn_Model()\n",
    "bn_criterion = torch.nn.CrossEntropyLoss()\n",
    "bn_optimizer = torch.optim.SGD(bn_model.parameters(), lr=0.1)\n",
    "\n",
    "bn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "bn_model.train()\n",
    "for epoch in range(training_epochs):\n",
    "  for X, Y in train_loader:\n",
    "    X = X.view(-1, 28 * 28).to(device)\n",
    "    Y = Y.to(device)\n",
    "\n",
    "    bn_optimizer.zero_grad()\n",
    "    bn_prediction = bn_model(X)\n",
    "    bn_loss = bn_criterion(bn_prediction, Y)\n",
    "    bn_loss.backward()\n",
    "    bn_optimizer.step()\n",
    "\n",
    "  print(\"Epoch : {}, loss : {}\".format(epoch, bn_loss))\n",
    "  \n",
    "# test\n",
    "bn_model.eval()\n",
    "with torch.no_grad():\n",
    "  X_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "  Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "  prediction = bn_model(X_test)\n",
    "  accuracy = (torch.argmax(prediction, 1) == Y_test).float().mean()\n",
    "  print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nn_Model(torch.nn.Module):\n",
    "      def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.linear1 = torch.nn.Linear(784,32,bias=True)\n",
    "    self.linear2 = torch.nn.Linear(32,32,bias=True)\n",
    "    self.linear3 = torch.nn.Linear(32,10,bias=True)\n",
    "    self.relu = torch.nn.ReLU()\n",
    "    self.model = torch.nn.Sequential(\n",
    "        self.linear1, self.relu,\n",
    "        self.linear2, self.relu,\n",
    "        self.linear3\n",
    "    ).to(device)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    return self.model(X)\n",
    "\n",
    "nn_model = Nn_Model()\n",
    "nn_criterion = torch.nn.CrossEntropyLoss()\n",
    "nn_optimizer = torch.optim.SGD(nn_model.parameters(), lr=0.1)\n",
    "nn_model\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "nn_model.train()\n",
    "for epoch in range(training_epochs):\n",
    "  for X, Y in train_loader:\n",
    "    X = X.view(-1, 28 * 28).to(device)\n",
    "    Y = Y.to(device)\n",
    "\n",
    "    nn_optimizer.zero_grad()\n",
    "    nn_prediction = nn_model(X)\n",
    "    nn_loss = nn_criterion(nn_prediction, Y)\n",
    "    nn_loss.backward()\n",
    "    nn_optimizer.step()\n",
    "  print(\"Epoch : {}, loss : {}\".format(epoch, nn_loss))\n",
    "  \n",
    "# test\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "  X_test = mnist_test.test_data.view(-1,28*28).float().to(device)\n",
    "  Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "  prediction = nn_model(X_test)\n",
    "  accuracy = (torch.argmax(prediction, 1) == Y_test).float().mean()\n",
    "  print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
