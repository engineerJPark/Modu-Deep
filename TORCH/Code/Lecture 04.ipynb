{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression\n",
    "\n",
    "Low Level implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[73,80,75],\n",
    "                             [93,88,93],\n",
    "                             [89,91,80],\n",
    "                             [96,98,100],\n",
    "                             [73,66,70]])\n",
    "y_train = torch.FloatTensor([[152],[185,],[180],[196],[142]])\n",
    "\n",
    "W = torch.zeros((3,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W,b], lr=0.0001)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    hypothesis = x_train.mm(W) + b # matmul or @\n",
    "\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {}, Cost: {:.6f}'.format(epoch, epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Level Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim # optimizing algorith 사용\n",
    "import torch.nn as nn # Module 상속\n",
    "import torch.nn.functional as F # loss function 사용\n",
    "\n",
    "# forward() : hypothesis 반환\n",
    "# backward() : gradient 계산\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1) # 3개의 feature input과 1개의 feature output(그냥 class를 물어보는 것이므로)\n",
    "        \n",
    "    def forward(self, x): # linear를 반환\n",
    "        return self.linear(x)\n",
    "    \n",
    "x_train = torch.FloatTensor([[73,80,75],\n",
    "                             [93,88,93],\n",
    "                             [89,91,80],\n",
    "                             [96,98,100],\n",
    "                             [73,66,70]])\n",
    "y_train = torch.FloatTensor([[152],[185,],[180],[196],[142]])\n",
    "\n",
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr =0.00001) # parameter를 받을 때는 model.paramter로 받는다.\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs + 1):\n",
    "    hypothesis = model(x_train)\n",
    "    cost = F.mse_loss(hypothesis, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {}, Cost: {:.6f}'.format(\n",
    "          epoch, epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn. Module에 관해서는 \n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "그냥 pytorch에서 neural network 정의할 때 반드시 상속시키는 class 이다.\n",
    "\n",
    "nn.Linear에 관해서는\n",
    "https://pytorch.org/docs/1.9.1/generated/torch.nn.Linear.html\n",
    "https://m.blog.naver.com/fbfbf1/222480437930\n",
    "feature의 개수만 받고 sample의 개수에 대해서는 신경쓰지 않는다.\n",
    "\n",
    "model.paramter에 관해서는 \n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters\n",
    "그냥 optimizer에 parameter를 전달하기 위해서 사용한다고 생각하면 되겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Big Data\n",
    "\n",
    "이제는 많은 데이터를 받아오는 경우를 생각해보자\n",
    "많은 데이터를 사용해야한다. 그러면 일부분씩 나눠서 학습한다.\n",
    "\n",
    "전체 데이터를 사용할 때보다는 좀더 거친 epoch-loss graph를 보인다.\n",
    "\n",
    "![](./image/2022-01-16-17-59-16.png)\n",
    "\n",
    "Minibatch Stochastic Gradient Descent를 하면 다음과 같이 descent가 불균일하게 된다. 거시적으로만 Cost가 줄어드는 셈.\n",
    "\n",
    "![](./image/2022-01-16-18-00-22.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# 그동안은 다음과 같이 Data를 불러오거나 생성해서 사용하였다.\n",
    "\n",
    "x_train = torch.FloatTensor([[79,88,75],\n",
    "                             [93,88,93],\n",
    "                             [89,91,90],\n",
    "                             [96,98,100],\n",
    "                             [73,66,70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞으로 큰 데이터는 다음과 같이 한다.\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[79,88,75],\n",
    "                       [93,88,93],\n",
    "                       [89,91,90],\n",
    "                       [96,98,100],\n",
    "                       [73,66,70]]\n",
    "        self.y_data = [[152],[185],[180],[196],[142]]\n",
    "    \n",
    "    def __len__(self): # 이 데이터셋의 총 데이터 수\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self.x_data): # idx데이터 반환\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "\n",
    "dataset = CustomDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size = 2, # batch size 정의. 메모리의 효율적 사용을 위해 2의 제곱수로 설정\n",
    "                        shuffle=True) # epoch마다 데이터셋을 섞는다. 데이터가 학습되는 순서 바꿈."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate(dataloader) : minibatch 인덱스와 데이터를 받는다.\n",
    "# len(dataloader) : 한 epoch당 minibatch의 개수이다.\n",
    "\n",
    "nb_epoch = 20\n",
    "for epoch in range(nb_epoch + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        prediction = model(x_train)\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epoch, batch_idx + 1, len(dataloader), cost.item\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enumerate()에 대해서는 다음 링크로 확인한다.\n",
    "\n",
    "https://wikidocs.net/16045\n",
    "\n",
    "위의 경우 enumerate의 결과는 (idx, (x_train, y_train))일 것이다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
