{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntorch.manual_seed(777)\nif device == 'cuda':\n    torch.cuda.manual_seed_all(777)\n\n# image 3 * 224 * 224 assumed\nclass VGG(nn.Module): # use this for CIFAR10 datasets\n    def __init__(self, layer_list, num_class=10):\n        super().__init__()\n        self.vgg_layer_list = []\n        self.initial_channel = 3\n        for m in layer_list:\n            if m == 'M':\n                self.vgg_layer_list.append(nn.MaxPool2d(2))\n            else:\n                self.vgg_layer_list += [\n                    nn.Conv2d(self.initial_channel, m, kernel_size=3, stride=1, padding=1),\n                    nn.BatchNorm2d(m),\n                    nn.ReLU(inplace=True)]\n                self.initial_channel = m\n                \n        self.convnet = nn.Sequential(*self.vgg_layer_list)\n        self.avgpool = nn.AdaptiveAvgPool2d(7) # 뭘까?????\n        self.FClayer = nn.Sequential(\n            nn.Linear(512*7*7, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(4096, num_class) # softmax skip\n        )\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n                else:\n                    pass # 수정\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant(m.weight, 1)\n                nn.init.constant(m.bias, 0)\n            else:\n                pass\n    \n    def forward(self, x):\n        out = self.avgpool(self.convnet(x))\n        out = out.view(out.shape[0], -1)\n        out = self.FClayer(out)\n        return out\n                \nlayer_list = {\n    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], #8 + 3 =11 == vgg11\n    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], # 10 + 3 = vgg 13\n    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], #13 + 3 = vgg 16\n    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'], # 16 +3 =vgg 19\n    'custom' : [64,64,64,'M',128,128,128,'M',256,256,256,'M']\n}\n\nVGGnet = VGG(layer_list['D'], 10).to(device)\nVGGnet","metadata":{"id":"EnSPNWHYTeXJ","outputId":"4ec700d4-e027-4866-c107-1c9302137a95","execution":{"iopub.status.busy":"2022-01-28T01:31:08.044809Z","iopub.execute_input":"2022-01-28T01:31:08.045095Z","iopub.status.idle":"2022-01-28T01:31:09.281789Z","shell.execute_reply.started":"2022-01-28T01:31:08.045065Z","shell.execute_reply":"2022-01-28T01:31:09.281103Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trans = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n])\n\ntrainset = torchvision.datasets.CIFAR10(root='.CIFAR10', train=True,\n                                              transform=trans, download=True)\ntestset = torchvision.datasets.CIFAR10(root='.CIFAR10', train=False,\n                                              transform=trans, download=True)\n\ndata_loader_train = DataLoader(trainset, batch_size=512,\n                               shuffle=True, drop_last=True)\ndata_loader_test = DataLoader(testset, batch_size=32,\n                               shuffle=False, drop_last=True)\n\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.SGD(VGGnet.parameters(), lr=0.001, momentum=0.9)\nlr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9) # learning rate를 변경해주는 scheduler","metadata":{"id":"wwdXelIDrE65","outputId":"1548a34b-db2b-43c5-9884-890f17db9256","execution":{"iopub.status.busy":"2022-01-28T01:31:09.283542Z","iopub.execute_input":"2022-01-28T01:31:09.283793Z","iopub.status.idle":"2022-01-28T01:31:10.840480Z","shell.execute_reply.started":"2022-01-28T01:31:09.283757Z","shell.execute_reply":"2022-01-28T01:31:10.839721Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(1,3,224,224).to(device)\nVGGnet(x)","metadata":{"id":"ikD-MbOYpcoR","outputId":"40562dcb-4942-47c4-9f93-126d176c6c0e","execution":{"iopub.status.busy":"2022-01-28T01:31:10.841729Z","iopub.execute_input":"2022-01-28T01:31:10.841970Z","iopub.status.idle":"2022-01-28T01:31:10.858083Z","shell.execute_reply.started":"2022-01-28T01:31:10.841937Z","shell.execute_reply":"2022-01-28T01:31:10.857342Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def test_model(model, data_loader_test):\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        model.eval()\n        for data in data_loader_test:\n            images, labels = data\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            out = model(images)\n            total += labels.size(0)\n            correct += (torch.argmax(out, 1) == labels).sum().item()\n        print('Accuracy : {}'.format(100 * correct / total))","metadata":{"execution":{"iopub.status.busy":"2022-01-28T01:31:10.859949Z","iopub.execute_input":"2022-01-28T01:31:10.860196Z","iopub.status.idle":"2022-01-28T01:31:10.865930Z","shell.execute_reply.started":"2022-01-28T01:31:10.860162Z","shell.execute_reply":"2022-01-28T01:31:10.865076Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train_model(model, data_loader_train, data_loader_test, optimizer, criterion, lr_schedule=None):\n    model.train()\n    for epoch in range(30):\n        running_loss = 0.0\n        lr_sche.step() # update learning rate\n        \n        for i, data in enumerate(data_loader_train):\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            optimizer.zero_grad()\n\n            hypothesis = model(inputs)\n            loss = criterion(hypothesis, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item() / 512\n        print(\"Epoch : %d, Loss : %f\"%(epoch + 1, running_loss))\n        running_loss = 0\n        test_model(model, data_loader_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T01:31:10.867298Z","iopub.execute_input":"2022-01-28T01:31:10.867545Z","iopub.status.idle":"2022-01-28T01:31:10.876937Z","shell.execute_reply.started":"2022-01-28T01:31:10.867513Z","shell.execute_reply":"2022-01-28T01:31:10.876255Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_model(VGGnet, data_loader_train, data_loader_test, optimizer, criterion, lr_sche)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T01:31:10.878317Z","iopub.execute_input":"2022-01-28T01:31:10.878629Z","iopub.status.idle":"2022-01-28T01:48:02.426386Z","shell.execute_reply.started":"2022-01-28T01:31:10.878596Z","shell.execute_reply":"2022-01-28T01:48:02.425647Z"},"trusted":true},"execution_count":14,"outputs":[]}]}